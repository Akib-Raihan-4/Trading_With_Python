{"cells":[{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import yfinance as yf\n","import pandas as pd\n","\n","def download_top_stocks_by_volume():\n","    # Download S&P 500 ticker symbols\n","    snp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n","    data = pd.read_html(snp500_url)\n","    sp500_tickers = data[0]['Symbol'].tolist()\n","\n","    # Create an empty dictionary to store ticker symbols and their volumes\n","    volumes = {}\n","\n","    # Defining Start & End Date\n","    start_date = \"2010-01-01\"\n","    end_date = \"2021-01-01\"\n","\n","    # Iterate through each ticker symbol and get its historical data\n","    for ticker in sp500_tickers:\n","        try:\n","            data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n","            volumes[ticker] = data['Volume'].mean()  # Use mean volume for simplicity\n","        except Exception as e:\n","            print(f\"Error downloading data for {ticker}: {e}\")\n","\n","    # Sort the dictionary by volume in descending order\n","    sorted_volumes = sorted(volumes.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Take the top 100 stocks\n","    top_100_tickers = [ticker[0] for ticker in sorted_volumes[:100]]\n","\n","    # Download historical data for the top 100 stocks\n","    for ticker in top_100_tickers:\n","        try:\n","            data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n","            data.to_csv(f\"./SnP_100/{ticker}.csv\")\n","            # Do something with the data, e.g., save it to a file or process it further\n","            print(f\"Downloaded data for {ticker}\")\n","        except Exception as e:\n","            print(f\"Error downloading data for {ticker}: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["download_top_stocks_by_volume()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","1 Failed download:\n","['BRK.B']: Exception('%ticker%: No timezone found, symbol may be delisted')\n","\n","1 Failed download:\n","['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 2010-01-01 -> 2021-01-01)')\n","\n","1 Failed download:\n","['CEG']: Exception(\"%ticker%: Data doesn't exist for startDate = 1262322000, endDate = 1609477200\")\n","\n","1 Failed download:\n","['GEHC']: Exception(\"%ticker%: Data doesn't exist for startDate = 1262322000, endDate = 1609477200\")\n","\n","1 Failed download:\n","['KVUE']: Exception(\"%ticker%: Data doesn't exist for startDate = 1262322000, endDate = 1609477200\")\n","\n","1 Failed download:\n","['VLTO']: Exception(\"%ticker%: Data doesn't exist for startDate = 1262322000, endDate = 1609477200\")\n"]}],"source":["snp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n","data = pd.read_html(snp500_url)\n","sp500_tickers = data[0]['Symbol'].tolist()\n","\n","volumes = {}\n","\n","# Defining Start & End Date\n","start_date = \"2010-01-01\"\n","end_date = \"2021-01-01\"\n","\n","# Iterate through each ticker symbol and get its historical data\n","for ticker in sp500_tickers:\n","    try:\n","        data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n","        volumes[ticker] = data['Volume'].mean()  # Use mean volume for simplicity\n","    except Exception as e:\n","        print(f\"Error downloading data for {ticker}: {e}\")\n","\n","# Sort the dictionary by volume in descending order\n","sorted_volumes = sorted(volumes.items(), key=lambda x: x[1], reverse=True)\n","\n","# Take the top 100 stocks\n","top_100_tickers = [ticker[0] for ticker in sorted_volumes[:100]]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["['AAPL',\n"," 'BAC',\n"," 'TSLA',\n"," 'AMZN',\n"," 'GOOGL',\n"," 'GOOG',\n"," 'AMD',\n"," 'T',\n"," 'ABNB',\n"," 'AAL',\n"," 'AMAT',\n"," 'MO',\n"," 'ABT',\n"," 'AIG',\n"," 'ABBV',\n"," 'BK',\n"," 'AES',\n"," 'AXP',\n"," 'BAX',\n"," 'AFL',\n"," 'BKR',\n"," 'BBWI',\n"," 'APA',\n"," 'ADM',\n"," 'AMGN',\n"," 'ADBE',\n"," 'A',\n"," 'ANET',\n"," 'APH',\n"," 'AEP',\n"," 'ACN',\n"," 'ALL',\n"," 'MMM',\n"," 'ADI',\n"," 'AKAM',\n"," 'ADSK',\n"," 'BALL',\n"," 'AMT',\n"," 'ADP',\n"," 'APTV',\n"," 'ACGL',\n"," 'ALK',\n"," 'AEE',\n"," 'AON',\n"," 'AOS',\n"," 'AMCR',\n"," 'AMP',\n"," 'APD',\n"," 'LNT',\n"," 'AME',\n"," 'ALB',\n"," 'AXON',\n"," 'AWK',\n"," 'ALGN',\n"," 'AJG',\n"," 'AVY',\n"," 'AVB',\n"," 'ALLE',\n"," 'AIZ',\n"," 'ARE',\n"," 'ATO',\n"," 'ANSS',\n"," 'AZO',\n"," 'BRK.B',\n"," 'BF.B',\n"," 'CSCO',\n"," 'C',\n"," 'CMCSA',\n"," 'KO',\n"," 'BSX',\n"," 'BMY',\n"," 'SCHW',\n"," 'COP',\n"," 'CCL',\n"," 'CVX',\n"," 'CARR',\n"," 'BA',\n"," 'CAT',\n"," 'BBY',\n"," 'CF',\n"," 'CFG',\n"," 'CAG',\n"," 'CTSH',\n"," 'BX',\n"," 'CNP',\n"," 'CL',\n"," 'COF',\n"," 'CNC',\n"," 'CDNS',\n"," 'CAH',\n"," 'BWA',\n"," 'AVGO',\n"," 'CBRE',\n"," 'CMS',\n"," 'CMA',\n"," 'CI',\n"," 'CPB',\n"," 'COR',\n"," 'KMX',\n"," 'CME']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["top_100_tickers"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def OHLCV(data, stock_name):\n","    data = data.drop('Adj Close', axis = 1)\n","\n","    upDatedColumns = {'Date': 'date', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}\n","    data.rename(columns = upDatedColumns , inplace = True)\n","    data['volume'] = data['volume'].astype(float)\n","    \n","    data.iloc[:, 1:] = data.iloc[:, 1:].apply(lambda x: x.astype(float))\n","\n","    data['timestamp'] = pd.to_datetime(data['date'])\n","    data = data.reindex(columns =['timestamp', 'open','high', 'low', 'close', 'volume'])\n","    data['timestamp'] = data['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n","    data = data.sort_values(by='timestamp' , ascending = True)\n","    data = data.reset_index(drop = True)\n","\n","    data = data.groupby('timestamp').last().reset_index()\n","\n","    data.set_index('timestamp',inplace = True)\n","    data.to_csv(f\"./daily/{stock_name}.csv\")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["name = top_100_tickers[0]\n","\n","data = pd.read_csv(f\"./SnP_100/{name}.csv\")\n","OHLCV(data, name)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["for ticker in top_100_tickers:\n","    name = ticker\n","    data = pd.read_csv(f\"./SnP_100/{ticker}.csv\")\n","    OHLCV(data,name)\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["csvdir <no ingestions>\n","quandl 2023-12-26 11:48:49.209579\n","quantopian-quandl <no ingestions>\n","spStocks 2023-12-26 11:52:07.743594\n","spStocks 2023-12-26 11:49:31.893752\n","spStocks 2023-12-14 11:07:17.565899\n","spStocks 2023-12-14 08:41:15.735819\n","spStocks 2023-12-14 08:41:01.870597\n","spStocks 2023-12-14 08:40:45.601679\n","spStocks 2023-12-14 08:39:31.567706\n","spStocks 2023-12-13 07:34:23.012887\n","spStocks 2023-12-13 07:33:25.680991\n","spStocks 2023-12-13 07:29:27.845620\n"]}],"source":["!zipline bundles"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2023-12-26T18:00:57+0600-INFO][zipline.data.bundles.core]\n"," Ingesting spStocks\n","\u001b[?25lLoading custom pricing data:   [####################################]  100%\u001b[?25h\n","\u001b[?25lMerging daily equity files:  [####################################]    \u001b[?25h\n"]}],"source":["!zipline ingest -b spStocks"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pandas as pd\n","data = pd.read_csv('./SnP_100/AAPL.csv')\n","OHLCV(data, \"AAPL\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":2}
